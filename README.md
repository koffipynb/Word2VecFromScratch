# Word2Vec From Scratch

This repository contains a project where we implemented the **Word2Vec algorithm from scratch** in Python. The project focuses on understanding and building the Word2Vec model without relying on pre-built libraries, providing hands-on insight into how word embeddings are generated and utilized for downstream tasks.

## üìÅ Project Files
- **`word2vec.ipynb`**  
  Contains the implementation of the Word2Vec model from scratch, including the Skip-gram architecture. This notebook covers:
  - Data preprocessing: Tokenization, vocabulary creation, and context extraction.
  - Custom loss functions for training.
  - Training the Word2Vec model to generate word embeddings.

- **`word2vec_classification.ipynb`**  
  Demonstrates the application of the trained Word2Vec embeddings in a classification task. This notebook includes:
  - Loading and utilizing embeddings generated by the Word2Vec model.
  - Implementing a classifier that leverages the word embeddings for a binary classification task.
  - Evaluation of classification accuracy and insights into model performance.

## üõ†Ô∏è Features
- **Custom Implementation**: Fully recoded Word2Vec model with Skip-gram.
- **Understanding Word Embeddings**: Visualizing and utilizing embeddings for downstream tasks.
- **Classification Pipeline**: Applying embeddings for binary classification to demonstrate their utility.

## üß† Learnings
This project was an excellent opportunity to:
- Dive deep into the inner workings of the Word2Vec model.
- Understand how context-based word representations are created and trained.
- Apply custom embeddings in a practical downstream task.
